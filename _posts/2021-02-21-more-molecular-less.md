---
layout: post
title:  "More molecular, less variable?"
date:   2021-02-21 16:37:01 +0300
categories:
---

Reductionism – the belief that you can know how a system works by understanding its component parts – has underlain the thinking of biological scientists for the past few centuries. It is based on the principle that all of biology can be explained in terms of chemistry and physics: we are all made of small molecules, therefore, if we understand how those molecules work and interact, we will understand how the whole organism functions. Based on this principle, then, we might expect to get more precise answers the closer to the molecular level we get, as biology starts to absorb more of the presumably exact properties of chemistry and physics. In other words, it would make sense to get more precise answers when we are closer to the fundamental particles that define us.

Being a student of physiology, the discipline sometimes hailed as the antidote to reductionism, I readily admit to having a top-down bias in my view of science, yet the above statements make intuitive sense to me. But is the assumption true – does variability decrease with more reductive approaches? This is what Sapolsky and Balt, both critical of reductionism as the exclusive approach to science, put to the test in a 1996 paper ([Sapolsky and Balt 1996][1]). They collected a sample of 265 studies that provided evidence to address a certain question – what role testosterone plays in aggression – from any level of organisation (subcellular to organismal) and assigned all quantitative data and corresponding measures of variation in the papers to the level that was studied. Using the coefficient of variation, they then quantified the overall variation at each of six different levels of organisation (whole organism, organ system, single organ, multi-cellular, single cell and sub-cellular) from the pooled data. In the end, what the authors concluded was that the reductionist intuition was wrong – variability remains constant as you go from the organismal to the subcellular level. And when you really think about it, why should biology be more variable if we zoom out than if we zoom in on an organism?

What surprised me was that studies performed at more reductive levels of organisation contained less quantitative data. In organism-level studies, 48% of tables and figures contained quantitative data with error terms, whereas this was only 8% for sub-cellular studies. More molecular experiments might be expected to produce lots of quantitative data because we are operating at a level closer to chemistry, which is traditionally associated more with equations and numbers. However, many of the traditional molecular approaches: gels, blots and various forms of imaging, of course actually generate visual data. How do we express variability in these types of non-quantitative results then? The answer of course is that we don’t express it at all – most papers include a statement saying that a blot or image is “representative” without any mention of how the other results turned out. I won’t go into the obviously problematic practice of choosing “representative” images, but wouldn’t there be a benefit to somehow capturing variation in these types of non-quantitative results as well?

### Is variability in the eye of the beholder?

In quantitative terms, the _amount_ of variability therefore appears to stay constant across different levels of organisation – but is that variability also _interpreted_ in the same way at the different levels? Sapolsky and Balt argue that more molecular disciplines might view variability in a more favourable light because they have come up with terminology to dress that variability in exciting terms. Molecular biology harbours a variety of terms to, for example, give credence to variability between DNA or amino acid sequences (terms like single nucleotide polymorphisms (SNPs) or non-conserved residues). This variability can therefore be discussed and analysed to inspire further study. In contrast, there is no fancy terminology for variability in more macro-level behaviours such as the speed at which a gazelle runs or the propensity of different humans to consume junk food when stressed. For this reason, the variation is more likely to be seen simply as ‘noise’.

There is certainly some truth to their argument, but I think it goes deeper than linguistics: variability at a more reductionist level is perceived, not just described, differently. SNPs are viewed as exciting information, as something to focus whole studies on, whereas variability in organism-level behaviours is often not regarded with similar excitement. I suspect that we care more about variability at the molecular level not because we are operating at a more reductive level per se, but because we feel that we can do something with the data. Knowing what SNPs a person has can give us actionable information regarding their susceptibility to a disease or their metabolism of a certain drug, whereas there is less we can do with information about how many scoops (or tubs?) of ice cream different people go through when a deadline looms on the horizon.

### The smaller the error bars, the better?

The underlying assumption in this paper – and in science more broadly – is that variable data is bad, that noise is a problem. But what we call ‘noise’ is increasingly recognised to serve critical functions in cells ([Eldar and Elowitz 2010][2]), for example in regulating gene expression ([Paulsson _et al._ 2000][3], [Maheshri and O'Shea 2007][4]) and contributing to signal transduction ([Kuwahara and Gao 2013][5], [Ladbury and Arold 2012][6]). You might argue that the noise referred to above – noise intrinsic to biology – is different from noise present in data, yet the measurements we make will inevitably incorporate both of these. Technical and biological variability is intertwined.

I would also argue that variability is actually an essential property of a healthy system. Just like a loss of variability in heart rate and other physiological parameters can be a sign of dysfunction in the human body ([Grocott 2013][7]), biological data with very little variability might similarly reflect an unhealthy system. So should we always be aiming for the least variation possible when we produce data? Don’t get me wrong, I fully believe that we need to improve reproducibility in science. But at the same time, if we aim to eliminate variability, we also lose potentially interesting findings lurking within the error bars. In a paper I came across recently, the authors started exploring the variability in oxygenation they had noticed across pancreatic islets and discovered a functional reserve of insulin producing cells ([Olsson and Carlsson 2011][8]). Without initially having set up the experiment so that it allowed for messy, variable readings, this would not have been discovered. Moreover, if we perform experiments on cells or animals that are as similar as possible in order to get consistent data, we limit the ability to generalise our findings to the whole population.

So how should we think about variability then? The way I see it, it does makes science more complicated, but it is also a space of possibility from which new ideas can be born.


### References

[1]: https://muse.jhu.edu/article/401203 "Sapolsky R, Balt S. (1996) Reductionism and variability in data: A meta-analysis.  Perspectives in biology and medicine. 39(2):193-203."
[2]: https://doi.org/10.1038/nature09326 "Eldar A, Elowitz MB. (2010) Functional roles for noise in genetic circuits. Nature. 467(7312):167-73."
[3]: https://doi.org/10.1073/pnas.110057697 "Paulsson J, Berg OG, Ehrenberg M. (2000) Stochastic focusing: fluctuation-enhanced sensitivity of intracellular regulation. Proceedings of the National Academy of Sciences. 97(13):7148-53."
[4]: https://doi.org/10.1146/annurev.biophys.36.040306.132705 "Maheshri N, O’Shea EK. (2007) Living with noisy genes: how cells function reliably with inherent variability in gene expression. Annual review of biophysics and biomolecular structure. 36."
[5]: https://doi.org/10.1038/srep02297 "Kuwahara H, Gao X. (2013) Stochastic effects as a force to increase the complexity of signaling networks. Scientific reports. 3(1):1-8."
[6]: https://doi.org/10.1016/j.tibs.2012.01.001 "Ladbury JE, Arold ST. (2012) Noise in cellular signaling pathways: causes and effects. Trends in biochemical sciences. 37(5):173-8."
[7]: https://doi.org/10.1186/2046-7648-2-9 "Grocott MP. (2013) Integrative physiology and systems biology: reductionism, emergence and causality. Extreme Physiology & Medicine. 2(1):9."
[8]: https://doi.org/10.2337/db09-0877 "Olsson R, Carlsson PO. (2011) A low-oxygenated subpopulation of pancreatic islets constitutes a functional reserve of endocrine cells. Diabetes. 60(8):2068-75."

Eldar A, Elowitz MB. (2010) Functional roles for noise in genetic circuits. _Nature_. 467(7312):167-73.

Grocott MP. (2013) Integrative physiology and systems biology: reductionism, emergence and causality. _Extreme Physiology & Medicine_. 2(1):9.

Kuwahara H, Gao X. (2013) Stochastic effects as a force to increase the complexity of signaling networks. _Scientific reports_. 3(1):1-8.

Ladbury JE, Arold ST. (2012) Noise in cellular signaling pathways: causes and effects. _Trends in biochemical sciences_. 37(5):173-8.

Maheshri N, O’Shea EK. (2007) Living with noisy genes: how cells function reliably with inherent variability in gene expression. _Annual review of biophysics and biomolecular structure_. 36.

Olsson R, Carlsson PO. (2011) A low-oxygenated subpopulation of pancreatic islets constitutes a functional reserve of endocrine cells. _Diabetes_. 60(8):2068-75.

Paulsson J, Berg OG, Ehrenberg M. (2000) Stochastic focusing: fluctuation-enhanced sensitivity of intracellular regulation. _Proceedings of the National Academy of Sciences_. 97(13):7148-53.

Sapolsky R, Balt S. (1996) Reductionism and variability in data: A meta-analysis.  _Perspectives in biology and medicine_. 39(2):193-203
